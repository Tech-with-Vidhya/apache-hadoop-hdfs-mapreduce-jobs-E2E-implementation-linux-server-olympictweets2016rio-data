# hadoop-hdfs-mapreduce-jobs-E2E-implementation-in-linux-server-for-olympictweets2016rio-data

This project is delivered as part of my Masters in Big Data Science (MSc BDS) Program for the module named “Big Data Processing” in Queen Mary University of London (QMUL), London, United Kingdom.  

This project includes the development of a MapReduce python script with and without combiner from scratch for a big data job for the “olympictweets2016rio” private large-sized multiple data files; containing a large collection of Twitter messages collected during the Rio 2016 Olympics from the Twitter Streaming API using keywords like #Rio2016 or #rioolympics.   

The data files are stored in the Hadoop HDFS and the jobs are executed in the Hadoop Cluster.   

Identified solutions to the questions namely: 
1. Day with the highest number of tweets 
2. Total number of malformed lines of input 
3. Average length of the tweets  
4. Average number of hashtags  

**NOTE:** Due to the data privacy and the data protection policy to be adhered by the students; the datasets and the solution related code are not exposed and updated in the GitHub public profile; in order to be compliant with the Queen Mary University of London (QMUL) policies.
